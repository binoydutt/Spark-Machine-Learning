{"cells":[{"cell_type":"code","source":["import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\ndata = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"True\").option(\"inferSchema\", \"true\").option(\"delimiter\",\",\").load(\"/FileStore/tables/train.csv\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\n\n\ndef parse(x):\n  if x is not None:\n    return 1.0\n  else:\n    return 0.0\n  \nmyfunc = udf(parse, DoubleType() )\n#https://stackoverflow.com/questions/41362295/sparkexception-values-to-assemble-cannot-be-null\ndef preprocessing(dat):\n  df_final = dat.withColumn('Cabind',myfunc(dat['Cabin']) ).drop('Cabin').where(dat['Age'].isNotNull()).where(dat['Embarked'].isNotNull()).where(dat['Age'].isNotNull())\n  df= df_final.select(['Pclass','Age','Parch','SibSp','Fare','Cabind','Embarked','Sex','Survived']).na.drop()\n  categoricalColumns = ['Sex','Embarked']\n  stages = [] # stages in our Pipeline\n  for categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n  label_stringIdx = StringIndexer(inputCol = \"Survived\", outputCol = \"label\")\n  stages += [label_stringIdx]\n  numericCols = [\"Pclass\", \"Parch\",\"SibSp\", \"Fare\",\"Cabind\"]\n  assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n  assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n  stages += [assembler]\n  cols = df.columns\n# Create a Pipeline.\n  pipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\n  pipelineModel = pipeline.fit(df)\n  dataset = pipelineModel.transform(df)\n  selectedcols = [\"label\", \"features\"]+cols\n  dataset = dataset.select(selectedcols)\n  \n  return dataset"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["dataset = preprocessing(data)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["dataset.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())\n\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Use test set here so we can measure the accuracy of our model on new data\n\npredictions = cvModel.transform(testData)\n# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\n#print evaluator.evaluate(predictions)\ndef getaccuracy(predictions):\n  predictionAndLabels = predictions.select(\"prediction\", \"label\")\n \n  print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))\n\n  \ngetaccuracy(predictions)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1,2,6,10])\n             .addGrid(dt.maxBins, [20,40,80])\n             .build())"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["print \"numNodes = \", cvModel.bestModel.numNodes\nprint \"depth = \", cvModel.bestModel.depth\n\n# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)# cvModel uses the best model found from the Cross Validation\n\ngetaccuracy(predictions)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())\n\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\ncvModel = cv.fit(trainingData) "],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\ngetaccuracy(predictions)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#confusion Matrix\nlabel_and_pred = predictions.select(\"label\", \"prediction\")\nlabel_and_pred.rdd.zipWithIndex().countByKey()\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n# create the trainer and set its parameters\nlayers = [8, 5, 4, 2]\nmlp = MultilayerPerceptronClassifier(seed=12)\nparamGrid = (ParamGridBuilder()\n             .addGrid(mlp.layers, [[8,5,4,2],[8,5,4,2],[8,5,4,2]])\n             .addGrid(mlp.maxIter, [20, 60 , 80 , 100])\n             .build())\n\ncv = CrossValidator(estimator=mlp, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\ncvModel = cv.fit(trainingData)\n# compute accuracy on the test set\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["predictions = cvModel.transform(testData)\ngetaccuracy(predictions)"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"Assignment_Final","notebookId":666325530285112},"nbformat":4,"nbformat_minor":0}
